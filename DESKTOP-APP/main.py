# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'main.ui'
#
# Created by: PyQt5 UI code generator 5.15.2
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import QThread, pyqtSignal, pyqtSlot, Qt
from PyQt5.QtGui import QPixmap
from PyQt5.QtWidgets import QWidget, QDesktopWidget, QApplication, QInputDialog, QFileDialog

from shutil import copyfile
import pathlib
import os
import cv2
import face_recognition
import numpy as np
import logging
from datetime import datetime
import requests

from splash_screen import Ui_SplashScreen
from login_screen import Ui_LoginScreen
from main_screen import Ui_MainScreen

# ==> GLOBALS
counter = 0
app_path = str(pathlib.Path(__file__).parent.absolute())
face_cascade = cv2.CascadeClassifier('cascades/data/haarcascade_frontalface_alt2.xml')
usingOpenCv2 = False

# Initialize some variables
known_face_images = []
known_face_encodings = []
known_face_names = []

face_encodings = []
face_names = []

isLoadingData = True
isRecognizeOn = False
isRecognizeTempOn = False

# api-endpoint
URL = "http://localhost:8080"


def getImageOfUserFromDir(dirPath):
    for root, dirs, files in os.walk(dirPath):
        path = root.split(os.sep)
        for file in files:
            known_face_images.append(str(file))


def encodingDatasets():
    global isLoadingData
    isLoadingData = True
    getImageOfUserFromDir(app_path + "\\images\\")
    for image_names in known_face_images:
        image = face_recognition.load_image_file(f".\images\{image_names}")
        image_encoding = face_recognition.face_encodings(image)
        known_face_names.append(image_names[0: str(image_names).rfind(".")])
        known_face_encodings.append(image_encoding[0])
    isLoadingData = False


encodingDatasets()


def startRecognize():
    global isRecognizeOn
    global isRecognizeTempOn
    isRecognizeOn = True
    isRecognizeTempOn = True


def stopRecognize():
    global isRecognizeOn
    global isRecognizeTempOn
    isRecognizeOn = False
    isRecognizeTempOn = False


class QTextEditLogger(logging.Handler):
    def __init__(self, parent):
        super().__init__()
        self.widget = QtWidgets.QPlainTextEdit(parent)
        self.widget.setGeometry(QtCore.QRect(3, 30, 491, 371))
        font = QtGui.QFont()
        font.setPointSize(9)
        self.widget.setFont(font)
        self.widget.setStyleSheet("border: none\n"
                                  "")
        self.widget.setReadOnly(True)
        self.widget.setObjectName("txt_logs")

    def emit(self, record):
        msg = self.format(record)
        self.widget.appendPlainText(msg)


class VideoThread(QThread):
    change_pixmap_signal = pyqtSignal(np.ndarray)

    def run(self):
        # capture from web cam
        cap = cv2.VideoCapture(0)

        global face_encodings
        global face_names
        global known_face_encodings
        global known_face_names
        global isRecognizeOn
        global isRecognizeTempOn

        frame = 0
        while True:
            ret, cv_img = cap.read()
            cv_img = cv2.flip(cv_img, 1)
            frame += 1
            if frame == 80:
                if isRecognizeOn:
                    isRecognizeTempOn = True
                frame = 0

            if usingOpenCv2:
                gray = cv2.cvtColor(cv_img, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)
                for (x, y, w, h) in faces:
                    color = (0, 235, 0)  # BGR 0-255
                    stroke = 2
                    end_cord_x = x + w
                    end_cord_y = y + h
                    cv2.rectangle(cv_img, (x, y), (end_cord_x, end_cord_y), color, stroke)
            else:
                # Resize frame of video to 1/4 size for faster face recognition processing
                small_frame = cv2.resize(cv_img, (0, 0), fx=0.25, fy=0.25)

                # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)
                rgb_small_frame = small_frame[:, :, ::-1]

                # Find all the faces and face encodings in the current frame of video
                face_locations = face_recognition.face_locations(rgb_small_frame)

                face_names = []
                if isRecognizeTempOn:
                    isRecognizeTempOn = False
                    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)
                    for face_encoding in face_encodings:
                        # See if the face is a match for the known face(s)
                        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
                        name = "Unknown"

                        # # If a match was found in known_face_encodings, just use the first one.
                        # if True in matches:
                        #     first_match_index = matches.index(True)
                        #     name = known_face_names[first_match_index]

                        # Or instead, use the known face with the smallest distance to the new face
                        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                        best_match_index = np.argmin(face_distances)
                        if matches[best_match_index]:
                            name = known_face_names[best_match_index]

                        face_names.append(name)

                # Display the results
                if isRecognizeTempOn:
                    for (top, right, bottom, left), name in zip(face_locations, face_names):
                        # Scale back up face locations since the frame we detected in was scaled to 1/4 size
                        top *= 4
                        right *= 4
                        bottom *= 4
                        left *= 4

                        # Draw a box around the face
                        cv2.rectangle(cv_img, (left, top), (right, bottom), (0, 0, 255), 2)

                        # Draw a label with a name below the face
                        cv2.rectangle(cv_img, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
                        font = cv2.FONT_HERSHEY_DUPLEX
                        cv2.putText(cv_img, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)
                        if name != "Unknown":
                            logging.info(f"Employee with ID: {name} has been detected!")
                        else:
                            logging.warning(f"A stranger has been detected! Saved to the warning database!")

                else:
                    for top, right, bottom, left in face_locations:
                        # Scale back up face locations since the frame we detected in was scaled to 1/4 size
                        top *= 4
                        right *= 4
                        bottom *= 4
                        left *= 4

                        # Draw a box around the face
                        cv2.rectangle(cv_img, (left, top), (right, bottom), (0, 0, 255), 2)

            if ret:
                self.change_pixmap_signal.emit(cv_img)


class MainScreen(QtWidgets.QMainWindow):
    global app_path

    def __init__(self):
        # init login screen window
        QtWidgets.QMainWindow.__init__(self)
        self.ui = Ui_MainScreen()
        self.ui.setupUi(self)

        # width and height of image capture by open cv
        self.disply_width = 495
        self.display_height = 371

        # REMOVE TITLE BAR
        self.center()

        # set event for btn
        self.setEvent()

        # add log text box
        #  logTextBox = QTextEditLogger(self.ui.gr_logs)
        # You can format what is printed to text box
        # logTextBox.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        #  logging.getLogger().addHandler(logTextBox)
        # You can control the logging level
        # logging.getLogger().setLevel(logging.DEBUG)

        # create the video capture thread
        self.thread = VideoThread()
        # connect its signal to the update_image slot
        self.thread.change_pixmap_signal.connect(self.update_image)
        # start the thread
        self.thread.start()

        self.show()

    def center(self):
        qr = self.frameGeometry()
        cp = QDesktopWidget().availableGeometry().center()
        qr.moveCenter(cp)
        self.move(qr.topLeft())

    def setEvent(self):
        self.ui.btn_browse.clicked.connect(self.choseFile)
        self.ui.btn_add.clicked.connect(self.addUser)
        self.ui.btn_start.clicked.connect(startRecognize)
        self.ui.btn_stop.clicked.connect(stopRecognize)

    def choseFile(self):
        file_dialog = QFileDialog(self)

        # the name filters must be a list
        file_filter = "Images (*.png *.jpg)"
        file_name = QFileDialog.getOpenFileNames(file_dialog, "Chose Images", "", file_filter)

        if file_name[0]:
            self.ui.txt_file_name.setPlainText(file_name[0][0][file_name[0][0].rfind("/"):])
            self.filePath = file_name[0][0]
            self.ext = file_name[0][0][file_name[0][0].rfind("."):]
        else:
            self.ui.txt_file_name.setPlainText("No file selected")
            self.filePath = ""

    def addUser(self):

        id = self.ui.txt_id.toPlainText()
        isContinue = False
        if id != "" and self.filePath:
            if os.path.isfile(app_path + f"\\images\\{id}" + ".png") or os.path.isfile(
                    app_path + f"\\images\\{id}" + ".jpg"):
                msb = QtWidgets.QMessageBox()
                msb.setIcon(QtWidgets.QMessageBox.Warning)
                msb.setText("This user has been found in the dataset!\nDo you want to "
                            "overwrite?")
                msb.setWindowTitle("Warning")
                msb.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.Cancel)
                returnValue = msb.exec()
                if returnValue == QtWidgets.QMessageBox.Yes:
                    if os.path.isfile(app_path + f"\\images\\{id}" + ".png"):
                        os.remove(app_path + f"\\images\\{id}" + ".png")
                    else:
                        os.remove(app_path + f"\\images\\{id}" + ".jpg")
                    isContinue = True
            else:
                isContinue = True
        else:
            QtWidgets.QMessageBox().warning(self, "Warning", "Please chose a image and input the ID!")
            return

        if isContinue:
            copyfile(self.filePath, app_path + f"\\images\\{id}{self.ext}")
            QtWidgets.QMessageBox().information(self, "Notification", "Add user to the dataset successfully!")

        self.ui.txt_file_name.setPlainText("")
        self.ui.txt_id.setPlainText("")

    @pyqtSlot(np.ndarray)
    def update_image(self, cv_img):
        """Updates the image_label with a new opencv image"""
        qt_img = self.convert_cv_qt(cv_img)
        self.ui.lb_cam.setPixmap(qt_img)
        if isRecognizeOn and face_names != []:
            now = datetime.now()
            dt_string = now.strftime("%d/%m/%Y %H:%M:%S")
            log_text = self.ui.txt_logs.toPlainText()
            for name in face_names:
                if name == "Unknown":
                    self.ui.txt_logs.appendPlainText(f"{dt_string} - Unauthorized person detected!")
                else:
                    if f"{dt_string} - Employee with ID: {name} has been detected!" not in log_text:
                        self.ui.txt_logs.appendPlainText(f"{dt_string} - Employee with ID: {name} has been detected!")
                        requests.get(f"{URL}/check/{name}")

    def convert_cv_qt(self, cv_img):
        """Convert from an opencv image to QPixmap"""
        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_image.shape
        bytes_per_line = ch * w
        convert_to_Qt_format = QtGui.QImage(rgb_image.data, w, h, bytes_per_line, QtGui.QImage.Format_RGB888)
        p = convert_to_Qt_format.scaled(self.disply_width, self.display_height, Qt.KeepAspectRatio)
        return QPixmap.fromImage(p)


class LoginScreen(QtWidgets.QMainWindow):
    def __init__(self):
        # init login screen window
        QtWidgets.QMainWindow.__init__(self)
        self.ui = Ui_LoginScreen()
        self.ui.setupUi(self)

        self.ui.btnLogin.clicked.connect(self.login)

        # REMOVE TITLE BAR
        self.setWindowFlag(QtCore.Qt.FramelessWindowHint)

        self.show()

    def login(self):
        QtWidgets.QMessageBox.question(
            self.ui.centralwidget, "Login", "Login Successfully")
        self.main = MainScreen()
        self.main.show()

        self.close()


class SplashScreen(QtWidgets.QMainWindow):
    def __init__(self):
        # init Splash screen
        QtWidgets.QMainWindow.__init__(self)
        self.ui = Ui_SplashScreen()
        self.ui.setupUi(self)

        # REMOVE TITLE BAR
        self.setWindowFlag(QtCore.Qt.FramelessWindowHint)
        self.setAttribute(QtCore.Qt.WA_TranslucentBackground)

        # DROP SHADOW EFFECT
        self.shadow = QtWidgets.QGraphicsDropShadowEffect(self)
        self.shadow.setBlurRadius(20)
        self.shadow.setXOffset(0)
        self.shadow.setYOffset(0)
        self.shadow.setColor(QtGui.QColor(0, 0, 0, 60))
        self.ui.dropShadowFrame.setGraphicsEffect(self.shadow)

        # QTIMER ==> START
        self.timer = QtCore.QTimer()
        self.timer.timeout.connect(self.progress)
        # TIMER IN MILLISECONDS
        self.timer.start(45)

        # CHANGE DESCRIPTION

        # Initial Text
        self.ui.label_description.setText(
            "<strong>WELCOME</strong> TO TPCOP SYSTEM")

        # Change Texts
        QtCore.QTimer.singleShot(1500, lambda: self.ui.label_description.setText(
            "<strong>LOADING</strong> DATABASE"))
        QtCore.QTimer.singleShot(3000, lambda: self.ui.label_description.setText(
            "<strong>LOADING</strong> USER INTERFACE"))

        self.show()

    def progress(self):
        global counter

        # SET VALUE TO PROGRESS BAR
        self.ui.progressBar.setValue(counter)

        # CLOSE SPLASH SCREE AND OPEN APP
        if counter > 100:
            # STOP TIMER
            self.timer.stop()

            # SHOW MAIN WINDOW
            self.login = LoginScreen()
            self.login.show()

            # CLOSE SPLASH SCREEN
            self.close()

        # INCREASE COUNTER
        counter += 1


if __name__ == "__main__":
    import sys

    app = QtWidgets.QApplication(sys.argv)

    window = MainScreen()

    sys.exit(app.exec_())
